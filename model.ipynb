{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPV6q8jagSX5lOVDGrp/J23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zambbo/CNN-DialectDetector/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msnE4Uhjy3Te",
        "outputId": "8374e98e-3ace-4210-be99-e29cc4370401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "kBVhaUJJ2fef"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2region={0:'gangwon', 1:'gyeongsang', 2:'jeonla', 3:'chungcheong', 4:'jeju'}\n",
        "region2index = {v:k for k,v in index2region.items()}"
      ],
      "metadata": {
        "id": "vnfeGjKBzCQW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 셋 구성 (small dataset)\n",
        "dataset_dir = '/content/drive/MyDrive/DialectDataset/small_dataset/'"
      ],
      "metadata": {
        "id": "SdpZJHsu3iVm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_dir = glob(dataset_dir)\n",
        "region_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt9xHJeX4eUj",
        "outputId": "4784c168-0d31-4903-f6a3-9899be3d0689"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/DialectDataset/small_dataset/']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in index2region.items():\n",
        "    exec(f\"{v}_dirs = glob(dataset_dir+'*_{v}/*')\")\n",
        "jeonla_dirs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmjloLpw4gjV",
        "outputId": "6bbcc773-05d3-4b9b-886d-11d3ef660e94"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000014',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000012',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000019',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000006',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000015',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000032',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000027',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000018',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000005',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000024']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tuple_data(dirs, max_num):\n",
        "    for i, region_dir in enumerate(dirs):\n",
        "        if i>=max_num:break\n",
        "        spectro_path = glob(region_dir+'/*_spectro.pickle')[0]\n",
        "        mfcc_path = glob(region_dir+'/*_mfcc.pickle')[0]\n",
        "        chroma_path = glob(region_dir+'/*_chroma.pickle')[0]\n",
        "        \n",
        "        with open(spectro_path, \"rb\") as f:\n",
        "            spectro = pickle.load(f)\n",
        "        with open(mfcc_path, \"rb\") as f:\n",
        "            mfcc = pickle.load(f)\n",
        "        with open(chroma_path, \"rb\") as f:\n",
        "            chroma = pickle.load(f)\n",
        "\n",
        "        if i == 0:\n",
        "            spectro_data = spectro\n",
        "            mfcc_data = mfcc\n",
        "            chroma_data = chroma\n",
        "        else:\n",
        "            spectro_data = np.concatenate([spectro_data,spectro], axis=0)\n",
        "            mfcc_data = np.concatenate([mfcc_data,mfcc], axis=0)\n",
        "            chroma_data = np.concatenate([chroma_data,chroma], axis=0)\n",
        "        \n",
        "    r_data = [(s,m,c) for s,m,c in zip(spectro_data,mfcc_data,chroma_data)]\n",
        "        \n",
        "    return r_data\n",
        "jeonla_data = make_tuple_data(jeonla_dirs, 2)\n",
        "chungcheong_data = make_tuple_data(chungcheong_dirs, 2)\n",
        "gyeongsang_data = make_tuple_data(gyeongsang_dirs, 2)\n",
        "jeju_data = make_tuple_data(jeju_dirs, 2)\n",
        "gangwon_data = make_tuple_data(gangwon_dirs, 2)\n",
        "\n",
        "print(\"data num: \", len(jeonla_data))\n",
        "print(\"tuple size\", len(jeonla_data[0]))\n",
        "print(\"spec shape\", jeonla_data[0][0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYUO22zQ8LVm",
        "outputId": "65734cc1-02e9-4b28-f43b-de7a76fdfcec"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data num:  197\n",
            "tuple size 3\n",
            "spec shape (201, 501)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jeonla_data = [(d, region2index['jeonla']) for d in jeonla_data]\n",
        "chungcheong_data = [(d, region2index['chungcheong']) for d in chungcheong_data]\n",
        "gangwon_data = [(d, region2index['gangwon']) for d in gangwon_data]\n",
        "jeju_data = [(d, region2index['jeju']) for d in jeju_data]\n",
        "gyeongsang_data = [(d, region2index['gyeongsang']) for d in gyeongsang_data]"
      ],
      "metadata": {
        "id": "xr6qlZ1YDxnq"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasumup = np.concatenate([jeonla_data, chungcheong_data, gangwon_data, jeju_data, gyeongsang_data], axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzqJhuPUE745",
        "outputId": "61f19281-9d42-4032-cee7-6022c216ae33"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(jeonla_data))\n",
        "print(len(jeonla_data[0]))\n",
        "print(len(jeonla_data[0][0]))\n",
        "print(jeonla_data[0][0][0].shape)\n",
        "print(jeonla_data[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R92zKFQGEXcq",
        "outputId": "9955f65a-c2ad-4ff3-ee2f-3e7bdc9b992c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "197\n",
            "2\n",
            "3\n",
            "(201, 501)\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        datas, label = self.data[idx]\n",
        "        spec, mfcc, chroma = datas\n",
        "\n",
        "        spec, mfcc, chroma = torch.tensor(spec, dtype=torch.float32), torch.tensor(mfcc, dtype=torch.float32), torch.tensor(chroma, dtype=torch.float32)\n",
        "        spec, mfcc, chroma = spec.unsqueeze(0), mfcc.unsqueeze(0), chroma.unsqueeze(0)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        data = (spec, mfcc, chroma)\n",
        "        return data, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "tW3F0sH5BOQG"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MultiModalDataset(datasumup)\n",
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGVp0a6QB_aX",
        "outputId": "b3732a81-f811-4a9f-88d5-58b3469f576a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "920"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0')"
      ],
      "metadata": {
        "id": "c13nI_24LS6L"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3,3), stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.seq1 = nn.Sequential(self.conv1, self.bn1, self.relu)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(3,3), stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.seq2 = nn.Sequential(self.conv2, self.bn2)\n",
        "        \n",
        "        self.down_flag = False\n",
        "        if in_channels != out_channels: self.down_flag = True\n",
        "\n",
        "        self.downsample = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1,1), stride=2, padding=0, bias=False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #print(x.shape)\n",
        "        y = self.seq1(x)\n",
        "        #print(y.shape)\n",
        "        y = self.seq2(y)\n",
        "        #print(y.shape)\n",
        "\n",
        "        if self.down_flag:\n",
        "            x = self.downsample(x)\n",
        "        \n",
        "        y = self.relu(y)\n",
        "        #print(x.shape)\n",
        "        #print(y.shape)\n",
        "        y += x\n",
        "\n",
        "        return y\n",
        "        "
      ],
      "metadata": {
        "id": "mj7PPfwSFgeo"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block = BasicBlock(1, 64, stride=2).to(device)"
      ],
      "metadata": {
        "id": "OwzN7ybAJ4KE"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "Ry9Np7gkMzAL"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "F7qba5DlRiIz"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (data, label) in enumerate(dataloader):\n",
        "\n",
        "    if i==1:break\n",
        "\n",
        "    data = data[0]#spec\n",
        "    data = data.to(device)\n",
        "    out = block(data)\n",
        "    print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6OCaoG4KKke",
        "outputId": "bc11c05b-4241-448f-d39a-d23bd1159936"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 201, 501])\n",
            "torch.Size([16, 64, 101, 251])\n",
            "torch.Size([16, 64, 101, 251])\n",
            "torch.Size([16, 64, 101, 251])\n",
            "torch.Size([16, 64, 101, 251])\n",
            "torch.Size([16, 64, 101, 251])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mSGg5_fTO4M",
        "outputId": "bd76032a-ba37-484c-c3db-f1e7d10124c3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-7.2468, -6.0161, -5.6971,  ..., -5.7139, -3.4103, -7.1196],\n",
              "        [-8.8140, -9.7790, -9.7790,  ..., -9.7790, -8.2538, -8.5428],\n",
              "        [-9.2004, -9.7790, -9.6664,  ..., -9.7790, -6.4552, -9.7790],\n",
              "        ...,\n",
              "        [-9.2318, -9.7487, -9.7790,  ..., -9.7790, -5.9848, -9.7790],\n",
              "        [-9.7790, -9.7790, -9.7790,  ..., -9.7790, -9.7444, -9.7790],\n",
              "        [-9.7790, -9.7790, -9.7790,  ..., -9.7790, -9.3316, -9.7790]],\n",
              "       device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, output_dim=1024, model_type='spec'):\n",
        "        super(ResNet18, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=(7,7), stride=2, padding=3)\n",
        "        self.BN1 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(3,3), stride=2, padding=1)\n",
        "\n",
        "        self.seq1 = nn.Sequential(self.conv1, self.BN1, self.pool1)\n",
        "\n",
        "        self.seq2 = nn.Sequential(BasicBlock(64,64), BasicBlock(64,64))\n",
        "        self.seq3 = nn.Sequential(BasicBlock(64,64), BasicBlock(64, 128, stride=2))\n",
        "        self.seq4 = nn.Sequential(BasicBlock(128,128), BasicBlock(128,128))\n",
        "        self.seq5 = nn.Sequential(BasicBlock(128,128), BasicBlock(128,256,stride=2))\n",
        "\n",
        "        if model_type=='spec':\n",
        "            self.fc1 = nn.Linear(256*13*32, output_dim)\n",
        "        elif model_type=='mfcc':\n",
        "            self.fc1 = nn.Linear(256*7*32, output_dim)\n",
        "        elif model_type=='chroma':\n",
        "            self.fc1 = nn.Linear(256*1*32, output_dim)\n",
        "\n",
        "\n",
        "        self.lastlayer = nn.Sequential(self.fc1, self.relu)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.seq1(x)\n",
        "        y = self.seq2(y)\n",
        "        y = self.seq3(y)\n",
        "        y = self.seq4(y)\n",
        "        y = self.seq5(y)\n",
        "        y = y.view(y.shape[0],-1)\n",
        "        y = self.lastlayer(y)\n",
        "\n",
        "        return y\n",
        "\n"
      ],
      "metadata": {
        "id": "RuUiC7NlJfyq"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalDialectClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim=1024, out_dim=5, learning_rate=0.01):\n",
        "        super(MultiModalDialectClassifier, self).__init__()\n",
        "\n",
        "        self.spec_res = ResNet18(1, model_type='spec')\n",
        "        self.mfcc_res = ResNet18(1, model_type='mfcc')\n",
        "        self.chroma_res = ResNet18(1, model_type='chroma')\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(1024*3, 512)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(512,out_dim)\n",
        "        self.lastlayer = nn.Sequential(self.fc1, self.relu, self.dropout, self.fc2)\n",
        "\n",
        "        self.loss_f = nn.CrossEntropyLoss()\n",
        "        self.optim = optim.AdamW(self.parameters(), lr=learning_rate)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        spec_x, mfcc_x, chroma_x = x\n",
        "\n",
        "        spec_y = self.spec_res(spec_x)\n",
        "        mfcc_y = self.mfcc_res(mfcc_x)\n",
        "        chroma_y = self.chroma_res(chroma_x)\n",
        "\n",
        "        y = torch.cat([spec_y, mfcc_y, chroma_y], dim=1)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        \n",
        "        y = self.lastlayer(y)\n",
        "        print(y.shape)\n",
        "        return y\n",
        "    \n",
        "    def train(self, train_loader, valid_loader, learning_rate, epochs, device):\n",
        "        self.train_accuracy = []\n",
        "        self.valid_accuracy = []\n",
        "        best_epoch = -1\n",
        "        best_acc = -1\n",
        "\n",
        "        self.train()\n",
        "        \n",
        "        for epoch in range(1, epochs+1):\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            start_time = time.time()\n",
        "            epoch_loss = 0.0\n",
        "            epoch_acc = 0.0\n",
        "\n",
        "            for batch_idx, (batch_data, batch_label) in enumerate(tqdm(train_loader)):\n",
        "                \n",
        "                spec, mfcc, chroma = batch_data\n",
        "                spec, mfcc, chroma = spec.to(device), mfcc.to(device), chroma.to(device)\n",
        "                batch_data = (spec, mfcc, chroma)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                pred = self.forward(batch_data)\n",
        "                loss = self.loss(pred, batch_label)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += loss\n",
        "\n",
        "                _, pred_indices = torch.max(pred, axis=1)\n",
        "                total += batch_data.shape[0]\n",
        "                correct += pred_indices.eq(batch_label).sum().item()\n",
        "            \n",
        "            epoch_loss /= len(train_loader)\n",
        "            epoch_acc = correct / total\n",
        "\n",
        "            if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_epoch = epoch\n",
        "            \n",
        "    def predict(self, test_loader, device):\n",
        "        self.eval()\n",
        "        labels = []\n",
        "        predicted = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (batch_data, batch_label) in enumerate(tqdm(test_loader)):\n",
        "\n",
        "                spec, mfcc, chroma = batch_data\n",
        "                spec, mfcc, chroma = spec.to(device), mfcc.to(device), chroma.to(device)\n",
        "                batch_data = (spec, mfcc, chroma)\n",
        "                \n",
        "                pred = self.forward(batch_data)\n",
        "\n",
        "                _, pred_indices = torch.max(pred, axis=1)\n",
        "\n",
        "                predicted.append(pred_indices.numpy())\n",
        "                labels.append(batch_label.numpy())\n",
        "\n",
        "        predicted = np.concatenate(predicted, axis=0)       \n",
        "        labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "        return predicted, labels\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "2bHMnPJIY87t"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiModalDialectClassifier().to(device)"
      ],
      "metadata": {
        "id": "JQR5-OSsWel5"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (data, label) in enumerate(dataloader):\n",
        "\n",
        "    if i==1:break\n",
        "\n",
        "    data = data\n",
        "    spec, mfcc, chroma = data\n",
        "    spec, mfcc, chroma = spec.to(device), mfcc.to(device), chroma.to(device)\n",
        "    data = (spec, mfcc, chroma)\n",
        "    out = model(data)\n",
        "    print(out.shape)\n",
        "    _, predict = torch.max(out, axis=1)\n",
        "    pred.eq\n",
        "    print(predict)\n",
        "    print(label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI9ng5dnWX9K",
        "outputId": "5a343cfe-3301-493e-def4-b36a4129ec6a"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 5])\n",
            "torch.Size([16, 5])\n",
            "tensor([1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "tensor([2, 4, 3, 3, 2, 0, 3, 4, 3, 3, 1, 0, 0, 4, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2])\n",
        "b = torch.tensor([1,2])"
      ],
      "metadata": {
        "id": "cdWzenz5WiXI"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQEcGVv0jZHa",
        "outputId": "3c09071f-272d-421e-fcb7-ff1d3bef6e43"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFh9v24MjawJ",
        "outputId": "a1e3dabf-aa1e-483c-db59-0d2daecd6c35"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0,)"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gn_vHltnkeKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}