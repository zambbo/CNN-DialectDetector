{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNx6uJB7ULTMANV9yxDFuhl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zambbo/CNN-DialectDetector/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msnE4Uhjy3Te",
        "outputId": "e3f4017f-4a08-4431-a0ea-8b63d2fc98f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "import os\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "kBVhaUJJ2fef"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2region={0:'gangwon', 1:'gyeongsang', 2:'jeonla', 3:'chungcheong', 4:'jeju'}\n",
        "region2index = {v:k for k,v in index2region.items()}\n",
        "region_shortening = ['GW','GS','JL','CC','JJ']\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "vnfeGjKBzCQW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 셋 구성 (small dataset)\n",
        "dataset_dir = '/content/drive/MyDrive/DialectDataset/small_dataset/'"
      ],
      "metadata": {
        "id": "SdpZJHsu3iVm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_dir = glob(dataset_dir)\n",
        "region_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt9xHJeX4eUj",
        "outputId": "90a846b3-f470-49c2-e247-e185b3f3d99d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/DialectDataset/small_dataset/']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in index2region.items():\n",
        "    exec(f\"{v}_dirs = glob(dataset_dir+'*_{v}/*')\")\n",
        "jeonla_dirs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmjloLpw4gjV",
        "outputId": "b675d09d-20ca-41c0-de77-54252344cc60"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000014',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000012',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000019',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000006',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000015',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000032',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000027',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000018',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000005',\n",
              " '/content/drive/MyDrive/DialectDataset/small_dataset/preprocessed_jeonla/DJDD20000024']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tuple_data(dirs, max_num):\n",
        "    for i, region_dir in enumerate(dirs):\n",
        "        if i>=max_num:break\n",
        "        spectro_path = glob(region_dir+'/*_spectro.pickle')[0]\n",
        "        mfcc_path = glob(region_dir+'/*_mfcc.pickle')[0]\n",
        "        chroma_path = glob(region_dir+'/*_chroma.pickle')[0]\n",
        "        \n",
        "        with open(spectro_path, \"rb\") as f:\n",
        "            spectro = pickle.load(f)\n",
        "        with open(mfcc_path, \"rb\") as f:\n",
        "            mfcc = pickle.load(f)\n",
        "        with open(chroma_path, \"rb\") as f:\n",
        "            chroma = pickle.load(f)\n",
        "\n",
        "        if i == 0:\n",
        "            spectro_data = spectro\n",
        "            mfcc_data = mfcc\n",
        "            chroma_data = chroma\n",
        "        else:\n",
        "            spectro_data = np.concatenate([spectro_data,spectro], axis=0)\n",
        "            mfcc_data = np.concatenate([mfcc_data,mfcc], axis=0)\n",
        "            chroma_data = np.concatenate([chroma_data,chroma], axis=0)\n",
        "    if max_num ==0:return []\n",
        "        \n",
        "    r_data = [(s,m,c) for s,m,c in zip(spectro_data,mfcc_data,chroma_data)]\n",
        "        \n",
        "    return r_data\n",
        "\n",
        "def make_tuple(max_num=2):\n",
        "    jeonla_data = make_tuple_data(jeonla_dirs, max_num)\n",
        "    chungcheong_data = make_tuple_data(chungcheong_dirs, max_num)\n",
        "    gyeongsang_data = make_tuple_data(gyeongsang_dirs, max_num)\n",
        "    jeju_data = make_tuple_data(jeju_dirs, max_num)\n",
        "    gangwon_data = make_tuple_data(gangwon_dirs, max_num)\n",
        "    return jeonla_data, chungcheong_data, gyeongsang_data, jeju_data, gangwon_data\n",
        "\n",
        "def print_data(r_data, region):\n",
        "    if len(r_data)==0: return\n",
        "    print(f\"{region} data num: \", len(r_data))\n",
        "    print(f\"{region} tuple size\", len(r_data[0]))\n",
        "    print(f\"{region} spec shape\", r_data[0][0].shape)\n",
        "\n",
        "jeonla_data, chungcheong_data, gyeongsang_data, jeju_data, gangwon_data = make_tuple(1000)\n",
        "print_data(jeonla_data, 'jeonla')\n",
        "print_data(chungcheong_data, 'chungcheong')\n",
        "print_data(gyeongsang_data, 'gyeongsang')\n",
        "print_data(jeju_data, 'jeju')\n",
        "print_data(gangwon_data, 'gangwon')"
      ],
      "metadata": {
        "id": "AYUO22zQ8LVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jeonla_data_l = []\n",
        "for data in jeonla_data:\n",
        "    y = [0,0,0,0,0]\n",
        "    y[region2index['jeonla']] = 1\n",
        "    jeonla_data_l.append((data,y))\n",
        "\n",
        "chungcheong_data_l = []\n",
        "for data in chungcheong_data:\n",
        "    y = [0,0,0,0,0]\n",
        "    y[region2index['chungcheong']] = 1\n",
        "    chungcheong_data_l.append((data,y))\n",
        "\n",
        "gyeongsang_data_l = []\n",
        "for data in gyeongsang_data:\n",
        "    y = [0,0,0,0,0]\n",
        "    y[region2index['gyeongsang']] = 1\n",
        "    gyeongsang_data_l.append((data,y))\n",
        "\n",
        "jeju_data_l = []\n",
        "for data in jeju_data:\n",
        "    y = [0,0,0,0,0]\n",
        "    y[region2index['jeju']] = 1\n",
        "    jeju_data_l.append((data,y))\n",
        "\n",
        "gangwon_data_l = []\n",
        "for data in gangwon_data:\n",
        "    y = [0,0,0,0,0]\n",
        "    y[region2index['gangwon']] = 1\n",
        "    gangwon_data_l.append((data,y))"
      ],
      "metadata": {
        "id": "xr6qlZ1YDxnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasumup = np.concatenate([jeonla_data_l, chungcheong_data_l, gangwon_data_l, jeju_data_l, gyeongsang_data_l], axis=0)"
      ],
      "metadata": {
        "id": "QzqJhuPUE745"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(jeonla_data))\n",
        "print(len(jeonla_data[0]))\n",
        "print(len(jeonla_data[0][0]))\n",
        "print(jeonla_data[0][0][0].shape)\n",
        "print(jeonla_data[0][1])"
      ],
      "metadata": {
        "id": "R92zKFQGEXcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "class MultiModalDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        datas, label = self.data[idx]\n",
        "        spec, mfcc, chroma = datas\n",
        "        spec, mfcc, chroma = normalize(spec), normalize(mfcc), normalize(chroma)\n",
        "        spec, mfcc, chroma = torch.tensor(spec, dtype=torch.float32), torch.tensor(mfcc, dtype=torch.float32), torch.tensor(chroma, dtype=torch.float32)\n",
        "        spec, mfcc, chroma = spec.unsqueeze(0), mfcc.unsqueeze(0), chroma.unsqueeze(0)\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        data = (spec, mfcc, chroma)\n",
        "        return data, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "tW3F0sH5BOQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MultiModalDataset(datasumup)\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "PGVp0a6QB_aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3,3), stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.seq1 = nn.Sequential(self.conv1, self.bn1, self.relu)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(3,3), stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.seq2 = nn.Sequential(self.conv2, self.bn2)\n",
        "        \n",
        "        self.down_flag = False\n",
        "        if in_channels != out_channels: self.down_flag = True\n",
        "\n",
        "        self.downsample = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1,1), stride=2, padding=0, bias=False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #print(x.shape)\n",
        "        y = self.seq1(x)\n",
        "        #print(y.shape)\n",
        "        y = self.seq2(y)\n",
        "        #print(y.shape)\n",
        "\n",
        "        if self.down_flag:\n",
        "            x = self.downsample(x)\n",
        "        \n",
        "        y = self.relu(y)\n",
        "        #print(x.shape)\n",
        "        #print(y.shape)\n",
        "        y = y + x\n",
        "\n",
        "        return y\n",
        "        "
      ],
      "metadata": {
        "id": "mj7PPfwSFgeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, output_dim=128, model_type='spec'):\n",
        "        super(ResNet18, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=(7,7), stride=2, padding=3)\n",
        "        self.BN1 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(3,3), stride=2, padding=1)\n",
        "\n",
        "        self.seq1 = nn.Sequential(self.conv1, self.BN1, self.pool1)\n",
        "\n",
        "        self.seq2 = nn.Sequential(BasicBlock(64,64), BasicBlock(64,64))\n",
        "        self.seq3 = nn.Sequential(BasicBlock(64,64), BasicBlock(64, 128, stride=2))\n",
        "        self.seq4 = nn.Sequential(BasicBlock(128,128), BasicBlock(128,128))\n",
        "        self.seq5 = nn.Sequential(BasicBlock(128,128), BasicBlock(128,256,stride=2))\n",
        "\n",
        "        self.avg_pool1 = nn.AdaptiveAvgPool2d((1,1))\n",
        "        # if model_type=='spec':\n",
        "        #     self.fc1 = nn.Linear(256*13*32, output_dim)\n",
        "        # elif model_type=='mfcc':\n",
        "        #     self.fc1 = nn.Linear(256*7*32, output_dim)\n",
        "        # elif model_type=='chroma':\n",
        "        #     self.fc1 = nn.Linear(256*1*32, output_dim)\n",
        "        self.fc1 = nn.Linear(256, output_dim)\n",
        "\n",
        "\n",
        "        self.lastlayer = nn.Sequential(self.fc1, self.relu)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.seq1(x)\n",
        "        y = self.seq2(y)\n",
        "        y = self.seq3(y)\n",
        "        y = self.seq4(y)\n",
        "        y = self.seq5(y)\n",
        "        y = self.avg_pool1(y)\n",
        "        y = y.view(y.shape[0],-1)\n",
        "        y = self.lastlayer(y)\n",
        "\n",
        "        return y\n",
        "\n"
      ],
      "metadata": {
        "id": "RuUiC7NlJfyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalDialectClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim=1024, out_dim=5, learning_rate=0.01, best_model_save_path=\"./best_model.pt\"):\n",
        "        super(MultiModalDialectClassifier, self).__init__()\n",
        "\n",
        "        self.best_model_save_path = best_model_save_path\n",
        "        self.spec_res = ResNet18(1, model_type='spec')\n",
        "        self.mfcc_res = ResNet18(1, model_type='mfcc')\n",
        "        self.chroma_res = ResNet18(1, model_type='chroma')\n",
        "        \n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(128*3, 128)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(128,out_dim)\n",
        "        self.lastlayer = nn.Sequential(self.fc1, self.dropout1, self.relu, self.fc2)\n",
        "\n",
        "        self.loss_f = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.AdamW(self.parameters(), lr=learning_rate)\n",
        "        #self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        spec_x, mfcc_x, chroma_x = x\n",
        "\n",
        "        spec_y = self.spec_res(spec_x)\n",
        "        mfcc_y = self.mfcc_res(mfcc_x)\n",
        "        chroma_y = self.chroma_res(chroma_x)\n",
        "\n",
        "        y = torch.cat([spec_y, mfcc_y, chroma_y], dim=1)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        \n",
        "        y = self.lastlayer(y)\n",
        "        #y = self.softmax(y)\n",
        "        #print(y.shape)\n",
        "        return y\n",
        "    \n",
        "    def train_(self, train_loader, val_loader, learning_rate, epochs, device):\n",
        "        self.train_accuracy = []\n",
        "        self.train_loss = []\n",
        "        self.val_accuracy = []\n",
        "        self.val_loss = []\n",
        "        self.pred_labels_train = []\n",
        "        self.real_labels_train = []\n",
        "        self.pred_labels_val = None\n",
        "        self.real_labels_val = None\n",
        "        best_epoch = -1\n",
        "        best_acc = -1 \n",
        "        \n",
        "        for epoch in range(1, epochs+1):\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            start_time = time.time()\n",
        "            epoch_loss = 0.0\n",
        "            epoch_acc = 0.0\n",
        "            self.train()\n",
        "\n",
        "            for batch_idx, (batch_data, batch_label) in enumerate(tqdm(train_loader)):\n",
        "                \n",
        "                spec, mfcc, chroma = batch_data\n",
        "                spec, mfcc, chroma = spec.to(device), mfcc.to(device), chroma.to(device)\n",
        "                batch_data = (spec, mfcc, chroma)\n",
        "                batch_label = batch_label.to(device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                pred = self.forward(batch_data) # (batch_size, 5)\n",
        "                loss = self.loss_f(pred, batch_label)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                _, pred_indices = torch.max(pred, axis=1)\n",
        "                total += batch_data[0].shape[0]\n",
        "                batch_label = torch.max(batch_label, axis=1)[1]\n",
        "                correct += pred_indices.eq(batch_label).sum().item()\n",
        "                \n",
        "                if epoch==epochs: #last epoch\n",
        "                    self.pred_labels_train.append(pred_indices)\n",
        "                    self.real_labels_train.append(batch_label)\n",
        "                #for p, l in zip(pred_indices, batch_label):\n",
        "                #    print(f\"predicted: {index2region[p.item()]} real:{index2region[l.item()]}\")\n",
        "            \n",
        "            end_time = time.time()\n",
        "            print(f\"epoch {epoch} time: {end_time-start_time}sec(s).\")\n",
        "            \n",
        "\n",
        "            epoch_loss /= len(train_loader)\n",
        "            self.train_loss.append(epoch_loss)\n",
        "            epoch_acc = correct / total\n",
        "            self.train_accuracy.append(epoch_acc)\n",
        "            print(f\"epoch {epoch} train accuracy: {epoch_acc}\")\n",
        "            print(f\"epoch {epoch} loss: {epoch_loss}\")  \n",
        "\n",
        "\n",
        "            predicted, labels, val_loss = self.predict(val_loader, device)\n",
        "            if epoch==epochs: #last epoch\n",
        "                self.pred_labels_val=predicted.cpu().numpy()\n",
        "                self.real_labels_val=labels.cpu().numpy()\n",
        "            val_acc = predicted.eq(labels).sum().item() / len(predicted)\n",
        "            print(f\"epoch {epoch} val accuracy: {val_acc}\")\n",
        "            print(f\"epoch {epoch} val loss: {val_loss}\")\n",
        "\n",
        "            if val_acc > epoch_acc:\n",
        "                best_acc = val_acc\n",
        "                best_epoch = epoch\n",
        "                torch.save(self.state_dict(), self.best_model_save_path)\n",
        "            \n",
        "            self.val_accuracy.append(val_acc)\n",
        "            self.val_loss.append(val_loss)\n",
        "        \n",
        "        self.pred_labels_train = torch.cat(self.pred_labels_train, dim=0)\n",
        "        self.real_labels_train = torch.cat(self.real_labels_train, dim=0)\n",
        "        self.pred_labels_train = self.pred_labels_train.cpu().numpy()\n",
        "        self.real_labels_train = self.real_labels_train.cpu().numpy()\n",
        "            \n",
        "            \n",
        "            \n",
        "        print(\"Finish!\")\n",
        "        \n",
        "        return best_acc, best_epoch\n",
        "            \n",
        "    def predict(self, test_loader, device):\n",
        "        self.eval()\n",
        "        labels = []\n",
        "        predicted = []\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (batch_data, batch_label) in enumerate(tqdm(test_loader)):\n",
        "\n",
        "                spec, mfcc, chroma = batch_data\n",
        "                spec, mfcc, chroma = spec.to(device), mfcc.to(device), chroma.to(device)\n",
        "                batch_data = (spec, mfcc, chroma)\n",
        "                batch_label = batch_label.to(device)\n",
        "                \n",
        "                pred = self.forward(batch_data)\n",
        "\n",
        "                _, pred_indices = torch.max(pred, axis=1)\n",
        "                loss = self.loss_f(pred, batch_label)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "\n",
        "                predicted.append(pred_indices)\n",
        "                batch_label = torch.max(batch_label, axis=1)[1]\n",
        "                labels.append(batch_label)\n",
        "        val_loss /= len(test_loader)\n",
        "        predicted = torch.cat(predicted, dim=0)\n",
        "        labels = torch.cat(labels, dim=0)\n",
        "\n",
        "        return predicted, labels, val_loss\n",
        "    \n",
        "    def plot(self, which):\n",
        "        \n",
        "        X = [i for i in range(1, len(self.train_accuracy) + 1)]\n",
        "        if which=='train_loss':\n",
        "            y = self.train_loss\n",
        "        elif which=='train_acc':\n",
        "            y = self.train_accuracy\n",
        "        elif which=='val_acc':\n",
        "            y = self.val_accuracy\n",
        "        elif which=='val_loss':\n",
        "            y = self.val_loss\n",
        "        elif which=='confusion_train':\n",
        "            ConfusionMatrixDisplay.from_predictions(self.real_labels_train, self.pred_labels_train, display_labels=region_shortening)\n",
        "            plt.title('train confusion matrix')\n",
        "            plt.savefig(f\"./model_{which}.png\")\n",
        "            plt.show()\n",
        "            return\n",
        "        elif which=='confusion_normalize_train':\n",
        "            ConfusionMatrixDisplay.from_predictions(self.real_labels_train, self.pred_labels_train, display_labels=region_shortening, normalize='true')\n",
        "            plt.title('train confusion matrix')\n",
        "            plt.savefig(f\"./model_{which}.png\")\n",
        "            plt.show()    \n",
        "            return        \n",
        "        elif which=='confusion_val':\n",
        "            ConfusionMatrixDisplay.from_predictions(self.real_labels_val, self.pred_labels_val, display_labels=region_shortening)\n",
        "            plt.title('val confusion matrix')\n",
        "            plt.savefig(f\"./model_{which}.png\")\n",
        "            plt.show()\n",
        "            return\n",
        "        elif which=='confusion_normalize_val':\n",
        "            ConfusionMatrixDisplay.from_predictions(self.real_labels_val, self.pred_labels_val, display_labels=region_shortening, normalize='true')\n",
        "            plt.title('val confusion matrix')\n",
        "            plt.savefig(f\"./model_{which}.png\")\n",
        "            plt.show()    \n",
        "            return   \n",
        "            \n",
        "\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.ylabel(which)\n",
        "        plt.title(which)\n",
        "        plt.plot(X, y, label=\"Train loss\")\n",
        "        plt.savefig(f\"./model_{which}.png\")\n",
        "        plt.show()\n",
        "\n",
        "    def getConvLayers(self, resnet):\n",
        "        weights = []\n",
        "        conv_layers = []\n",
        "        for i in range(len(resnet)):\n",
        "            if type(resnet[i]) == nn.Conv2d:\n",
        "                weights.append(resnet[i].weight)\n",
        "                conv_layers.append(resnet[i])\n",
        "            elif type(resnet[i]) == nn.Sequential:\n",
        "                for basic in resnet[i].children(): # basic block\n",
        "                    for in_basic in basic.children():\n",
        "                        if type(in_basic) == nn.Conv2d:\n",
        "                            weights.append(in_basic.weight)\n",
        "                            conv_layers.append(in_basic)\n",
        "                        if type(in_basic) == nn.Sequential:\n",
        "                            for in_basic_in_sequential in in_basic:\n",
        "                                if type(in_basic_in_sequential) == nn.Conv2d:\n",
        "                                    weights.append(in_basic_in_sequential.weight)\n",
        "                                    conv_layers.append(in_basic_in_sequential)            \n",
        "        return weights, conv_layers\n",
        "\n",
        "    def extractConvLayer(self):\n",
        "        children_ = list(self.children())\n",
        "        spec_resnet = list(children_[0].children())\n",
        "        mfcc_resnet = list(children_[1].children())\n",
        "        chroma_resnet = list(children_[2].children()) \n",
        "\n",
        "        spec_weights, spec_layers = self.getConvLayers(spec_resnet)\n",
        "        mfcc_weights, mfcc_layers = self.getConvLayers(mfcc_resnet)\n",
        "        chroma_weights, chroma_layers = self.getConvLayers(chroma_resnet)\n",
        "\n",
        "        self.spec_weights = spec_weights\n",
        "        self.spec_layers = spec_layers\n",
        "        self.mfcc_weights = mfcc_weights\n",
        "        self.mfcc_layers = mfcc_layers\n",
        "        self.chroma_weights = chroma_weights\n",
        "        self.chroma_layers = chroma_layers\n",
        "        \n",
        "        return spec_weights, spec_layers, mfcc_weights, mfcc_layers, chroma_weights, chroma_layers\n",
        "\n",
        "    def plotFilter(self, where='first', data_type='spec', when='before_train'):\n",
        "        if data_type == 'spec':\n",
        "            filters = self.spec_weights\n",
        "        elif data_type == 'mfcc':\n",
        "            filters = self.mfcc_weights\n",
        "        elif data_type == 'chroma':\n",
        "            filters = self.chroma_weights\n",
        "        x_len = 0\n",
        "        y_len = 0\n",
        "        if where=='first': # 64x1x7x7\n",
        "            filters = filters[0]\n",
        "            filters = filters[:,0,:,:]\n",
        "            plt.figure(figsize=(20,17))\n",
        "            x_len=8\n",
        "            y_len=8\n",
        "        elif where=='middle': # 128x64x3x3\n",
        "            filters = filters[16]\n",
        "            filters = filters[:,0,:,:]\n",
        "            plt.figure(figsize=(30,25))\n",
        "            x_len= 16\n",
        "            y_len= 8\n",
        "        elif where=='last': # 256x256x3x3\n",
        "            filters = filters[len(filters)-2]\n",
        "            filters = filters[:,0,:,:]\n",
        "            plt.figure(figsize=(40,32))\n",
        "            x_len=16\n",
        "            y_len=16\n",
        "        for i,filter in enumerate(filters):\n",
        "            plt.subplot(x_len, y_len, i+1)\n",
        "            plt.imshow(filter.detach().cpu(), cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.savefig(f\"./{data_type}_filter_{where}_{when}.png\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "    \n",
        "    def plotOriginalImage(self, data):\n",
        "        plt.imshow(data[0,:,:])\n",
        "        plt.show()\n",
        "\n",
        "    def plotFeatureMap(self, data, where='first', data_type='spec', when='before_train'):\n",
        "        if data_type == 'spec':\n",
        "            layers = self.spec_layers\n",
        "            x_len = 8\n",
        "            y_len = 8\n",
        "        elif data_type == 'mfcc':\n",
        "            layers = self.mfcc_layers\n",
        "            x_len = 16\n",
        "            y_len = 4\n",
        "        elif data_type == 'chroma':\n",
        "            layers = self.chroma_layers\n",
        "            x_len = 32\n",
        "            y_len = 2\n",
        "\n",
        "        if where=='first':\n",
        "            plt.figure(figsize=(20,17))\n",
        "            layer = layers[0]\n",
        "        elif where=='middle':\n",
        "            pass\n",
        "        elif where=='last':\n",
        "            pass\n",
        "        results = layer(data) # 64x?x251\n",
        "        for i, result in enumerate(results):\n",
        "            plt.subplot(x_len, y_len, i+1)\n",
        "            plt.imshow(result.detach().cpu())\n",
        "            plt.axis('off')\n",
        "        plt.savefig(f\"./{data_type}_feature_map_{where}_{when}.png\")\n",
        "        plt.show()\n",
        "        plt.close()      \n",
        "\n",
        "        \n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "2bHMnPJIY87t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiModalDialectClassifier().to(device)"
      ],
      "metadata": {
        "id": "JQR5-OSsWel5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spec_weights, spec_layers, mfcc_weights, mfcc_layers, chroma_weights, chroma_layers = model.extractConvLayer()"
      ],
      "metadata": {
        "id": "lyTI3TYIATdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.plotOriginalImage(dataset[0][0][0])"
      ],
      "metadata": {
        "id": "V6BvSNxCPYP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.plotFilter(where='first', data_type='spec', when='before_train')\n",
        "model.plotFilter(where='middle', data_type='mfcc', when='before_train')\n",
        "model.plotFilter(where='last', data_type='chroma', when='before_train')\n",
        "model.plotFilter(where='first', data_type='spec', when='before_train')\n",
        "model.plotFilter(where='middle', data_type='mfcc', when='before_train')\n",
        "model.plotFilter(where='last', data_type='chroma', when='before_train')\n",
        "model.plotFilter(where='first', data_type='spec', when='before_train')\n",
        "model.plotFilter(where='middle', data_type='mfcc', when='before_train')\n",
        "model.plotFilter(where='last', data_type='chroma', when='before_train')"
      ],
      "metadata": {
        "id": "_Un53dgzPeiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.plotFeatureMap(dataset[0][0][0].to(device), data_type='spec', when='before_train')\n",
        "model.plotFeatureMap(dataset[0][0][1].to(device), data_type='mfcc', when='before_train')\n",
        "model.plotFeatureMap(dataset[0][0][2].to(device), data_type='chroma', when='before_train')"
      ],
      "metadata": {
        "id": "jCKe5UuWCyt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(len(dataset)*0.8)\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "o9JomlP8osW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_(train_loader, test_loader, 0.0005, 15, device)"
      ],
      "metadata": {
        "id": "Gn_vHltnkeKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.plot('train_acc')\n",
        "model.plot('train_loss')\n",
        "model.plot('val_acc')\n",
        "model.plot('val_loss')\n",
        "model.plot('confusion_train')\n",
        "model.plot('confusion_normalize_train')\n",
        "model.plot('confusion_val')\n",
        "model.plot('confusion_normalize_val')"
      ],
      "metadata": {
        "id": "Trco3DXpFCbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spec_weights, spec_layers, mfcc_weights, mfcc_layers, chroma_weights, chroma_layers = model.extractConvLayer()"
      ],
      "metadata": {
        "id": "xXRY31AfHWXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.plotFilter(where='first', data_type='spec', when='after_train')\n",
        "model.plotFilter(where='middle', data_type='mfcc', when='after_train')\n",
        "model.plotFilter(where='last', data_type='chroma', when='after_train')\n",
        "model.plotFilter(where='first', data_type='spec', when='after_train')\n",
        "model.plotFilter(where='middle', data_type='mfcc', when='after_train')\n",
        "model.plotFilter(where='last', data_type='chroma', when='after_train')\n",
        "model.plotFilter(where='first', data_type='spec', when='after_train')\n",
        "model.plotFilter(where='middle', data_type='mfcc', when='after_train')\n",
        "model.plotFilter(where='last', data_type='chroma', when='after_train')"
      ],
      "metadata": {
        "id": "_s72jVmCHY0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.plotFeatureMap(dataset[0][0][0].to(device), data_type='spec', when='after_train')\n",
        "model.plotFeatureMap(dataset[0][0][1].to(device), data_type='mfcc', when='after_train')\n",
        "model.plotFeatureMap(dataset[0][0][2].to(device), data_type='chroma', when='after_train')"
      ],
      "metadata": {
        "id": "ixll7DxO0k0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Yl71OVhWR8v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}